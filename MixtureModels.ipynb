{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87238d03",
   "metadata": {},
   "source": [
    "Przed oddaniem zadania upewnij się, że wszystko działa poprawnie.\n",
    "**Uruchom ponownie kernel** (z paska menu: Kernel$\\rightarrow$Restart) a następnie\n",
    "**wykonaj wszystkie komórki** (z paska menu: Cell$\\rightarrow$Run All).\n",
    "\n",
    "Upewnij się, że wypełniłeś wszystkie pola `TU WPISZ KOD` lub `TU WPISZ ODPOWIEDŹ`, oraz\n",
    "że podałeś swoje imię i nazwisko poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e87c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35840c7b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "from itertools import cycle\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "from matplotlib import cm, patches, animation\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs, load_iris\n",
    "from tqdm.auto import trange\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "SEED = 13\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "CMAP = plt.cm.get_cmap(\"Dark2\", 8)\n",
    "\n",
    "\n",
    "def visualize(\n",
    "    points: Optional[np.ndarray] = None,\n",
    "    gaussians: Optional[List[Tuple[np.ndarray, np.ndarray, float]]] = None,\n",
    "    fig: Optional[plt.Figure] = None,\n",
    "    lims: int = 2,\n",
    ") -> Tuple[plt.Figure, Tuple[plt.Axes, ...]]:\n",
    "    \"\"\"Wizualizacja mikstury rozkładów normalnych.\n",
    "    \n",
    "    :param points: punkty 2D do wizualizacji\n",
    "    :param gaussians: lista krotek zawierających\n",
    "        macierz średnich, macierz kowariancji i mixing coefficients\n",
    "    :param fig: figure do rysowania\n",
    "    \"\"\"\n",
    "    handles = []\n",
    "    labels = []\n",
    "    if fig is None:\n",
    "        fig = plt.figure(figsize=(14, 5))\n",
    "        ax1 = fig.add_subplot(131)\n",
    "        ax2 = fig.add_subplot(132)\n",
    "        ax3 = fig.add_subplot(133, projection=\"3d\")\n",
    "    else:\n",
    "        ax1, ax2, ax3 = fig.get_axes()\n",
    "        ax1.clear()\n",
    "        ax2.clear()\n",
    "        ax3.clear()\n",
    "    \n",
    "    ax1.set_xlim(-lims, lims)\n",
    "    ax1.set_ylim(-lims, lims)\n",
    "    ax2.set_xlim(-lims, lims)\n",
    "    ax2.set_ylim(-lims, lims)\n",
    "    ax3.set_xlim(-lims, lims)\n",
    "    ax3.set_ylim(-lims, lims)\n",
    "    ax3.set_zlim(0, 0.5)\n",
    "        \n",
    "    if points is not None:\n",
    "        ax1.scatter(\n",
    "            points[:, 0], \n",
    "            points[:, 1],\n",
    "            s=4,\n",
    "            c=\"black\",\n",
    "            alpha=0.3, \n",
    "            marker=\"x\",\n",
    "            label=\"punkty\"\n",
    "        )\n",
    "        h, l = ax1.get_legend_handles_labels()\n",
    "        handles.extend(h)\n",
    "        labels.extend(l)\n",
    "    \n",
    "    if gaussians is not None:\n",
    "        cmap = cycle(CMAP(i) for i in range(len(gaussians)))\n",
    "        x = np.linspace(-lims, lims, 201)\n",
    "        y = np.linspace(-lims, lims, 201)\n",
    "        xs, ys = np.meshgrid(x, y)\n",
    "        positions = np.stack((xs, ys), axis=2)\n",
    "        zs = np.zeros_like(xs)\n",
    "\n",
    "        for idx, (mu, sigma, pi) in enumerate(gaussians):\n",
    "            c = next(cmap)\n",
    "            label = f\"komponent {idx}\"\n",
    "            rv = multivariate_normal(mu, sigma)\n",
    "            zk = rv.pdf(positions)\n",
    "            zs += pi * zk\n",
    "            contour = ax1.contour(xs, ys, zk, levels=3, colors=[c])\n",
    "            handles.append(patches.Patch(color=c, label=label))\n",
    "            labels.append(label)\n",
    "\n",
    "        ax2.contour(xs, ys, zs, levels=5)\n",
    "\n",
    "        ax3.plot_surface(xs, ys, zs, cmap=cm.coolwarm)\n",
    "        \n",
    "    fig.legend(handles, labels, loc='center left')\n",
    "    return fig, (ax1, ax2, ax3)\n",
    "\n",
    "\n",
    "def load_s_set() -> pd.DataFrame:\n",
    "    \"\"\"Load s_set dataset and standardize it.\"\"\"\n",
    "    data = pd.read_csv(\"s_set.csv\", delimiter=\"\\t\", names=[\"x\", \"y\"])\n",
    "    return (data - data.mean(axis=0)) / data.std(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-dressing",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ddc165bee1f39da2dd6c3d755417be3",
     "grade": false,
     "grade_id": "grading-var",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "grading = True  # ustaw na False!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-concord",
   "metadata": {},
   "source": [
    "# Modele mikstur\n",
    "\n",
    "Modele mikstury rozkładów normalnych (ang. *Gaussian Mixture Models*, **GMMs**) to jeden z *Latent Variable Models*. Zakładamy w nim, że obserwowane dane są generowane z mikstury skończonej liczby rozkładów normalnych o nieznanych parametrach. Poniżej przykład dla dwuwymiarowej przestrzeni.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-peeing",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f69d5c3c7568c19c350d67a20d1800a",
     "grade": false,
     "grade_id": "gaussian-mixture",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mu_0, sigma_0, pi_0 = np.array([-3.0, -2.0]), np.array([[3.0, 2.4],[2.4, 3.0]]), 0.3\n",
    "mu_1, sigma_1, pi_1 = np.array([0.0, 0.0]), np.array([[3.0, -2.0], [-2.0, 3.0]]), 0.5\n",
    "mu_2, sigma_2, pi_2 = np.array([2.0, 0.8]), np.array([[2.8, 1.8], [1.8, 2.8]]), 0.2\n",
    "\n",
    "points_0 = np.random.multivariate_normal(mu_0, sigma_0, size=int(100 * pi_0))\n",
    "points_1 = np.random.multivariate_normal(mu_1, sigma_1, size=int(100 * pi_1))\n",
    "points_2 = np.random.multivariate_normal(mu_2, sigma_2, size=int(100 * pi_2))\n",
    "\n",
    "visualize(\n",
    "    points=np.vstack((points_0, points_1, points_2)),\n",
    "    gaussians=[\n",
    "        (mu_0, sigma_0, pi_0), \n",
    "        (mu_1, sigma_1, pi_1),\n",
    "        (mu_2, sigma_2, pi_2)\n",
    "    ],\n",
    "    lims=6\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-hygiene",
   "metadata": {},
   "source": [
    "W przypadku modelu mikstur rozkładów normalnych mówimy o komponentach - zmiennych losowych o rozkładzie normalnym, z których każdy posiada:\n",
    "\n",
    "* **wektor średnich** $\\pmb{\\mu}$ określający środek rozkładu,\n",
    "* **macierz kowariancji** $\\pmb{\\Sigma}$ określający jego szerokość,\n",
    "* ***mixing coefficient*** $\\pi$ mówiący o prawdopodobieństwie danego komponentu (ich suma musi być równa $1$).\n",
    "\n",
    "Wówczas, dla wektora losowego $\\pmb{x}$ funkcja gęstości GMM zdefiniowana jest w następujący sposób:\n",
    "\n",
    "$$p\\left(\\pmb{x}\\right) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}\\left(\\pmb{x} \\mid \\pmb{\\mu}_k, \\pmb{\\Sigma}_k\\right)\\tag{pdf}$$\n",
    "\n",
    "Stosując GMM do *soft clustering* obliczamy prawdopodobieństwo posterior, że dany $n$-ty punkt należy do $k$-tego klastra - ***responsibility***.\n",
    "\n",
    "$$\\gamma\\left(z_{nk}\\right) := p\\left(z_n=k \\mid x_n, \\pmb{\\theta}\\right) = \\frac{p\\left(z_i=k \\mid \\pmb{\\theta}\\right)p\\left(x_n \\mid z_n=k, \\pmb{\\theta}\\right)}{\\sum_{k'=1}^K p\\left(z_n=k' \\mid \\pmb{\\theta}\\right)p\\left(x_n \\mid z_n=k', \\pmb{\\theta}\\right)} = \\frac{\\pi_k \\mathcal{N}\\left(\\pmb{x}_n \\mid \\pmb{\\mu}_k, \\pmb{\\Sigma}_k\\right)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}\\left(\\pmb{x}_n \\mid \\pmb{\\mu}_j, \\pmb{\\Sigma}_j\\right)}\\tag{gamma}$$\n",
    "\n",
    "## Expectation-Maximization\n",
    "\n",
    "Rozkład $(pdf)$ możemy rozważać jako rozkład brzegowy $p\\left(\\pmb{x} \\mid \\pmb{\\theta}\\right)$, wówczas możemy wyprowadzić uśredniony log-likelihood modelu dla zbioru danych $\\mathcal{D}$.\n",
    "\n",
    "$$ \\frac{1}{N} \\log p\\left(\\mathcal{D} \\mid \\pmb{\\pi}, \\pmb{\\mu}, \\pmb{\\Sigma}\\right) = \\frac{1}{N} \\sum_{n=1}^{N} \\log\\left(\\sum_{k=1}^K \\pi_k \\mathcal{N}\\left(x_n \\mid \\pmb{\\mu}_k, \\pmb{\\Sigma}_k\\right)\\right)\\tag{LL}$$\n",
    "\n",
    "Obliczając pochodne log-likelihood $(LL)$ (zgodnie z MLE) względem średnich $\\pmb{\\mu}_k$, kowariancji $\\pmb{\\Sigma}_k$, oraz używając *Lagrange coefficients* dla $\\pi_k$ można wyprowadzić wzory na estymację parametrów rozkładów:\n",
    "\n",
    "$$\\pmb{\\mu}_k^* = \\frac{1}{N_k} \\sum_{n=1}^N \\gamma(z_{nk}) \\pmb{x}_n\\tag{means}$$\n",
    "$$\\pmb{\\Sigma}_k^* = \\frac{1}{N_k} \\sum_{n=1}^N \\gamma\\left(z_{nk}\\right)\\left(\\pmb{x}_n - \\pmb{\\mu}_k\\right)\\left(\\pmb{x}_n - \\pmb{\\mu}_k\\right)^T\\tag{covariances}$$\n",
    "$$\\pi_k^* = \\frac{N_k}{N}\\tag{mixing coefs}$$\n",
    "$$N_k = \\sum_{n=1}^N \\gamma\\left(z_{nk}\\right)$$\n",
    "\n",
    "Równania te nie mają jawnej postaci ($\\gamma\\left(z_{nk}\\right)$ zależy od pozostałych parametrów w złożony sposób i odwrotnie). Możemy jednak iteracyjnie szukać rozwiązania, wykorzystująć algorytm ***Expectation-Maximization*** (EM).\n",
    "\n",
    "Algorytm EM dla GMM będzie postępował według następujących kroków:\n",
    "\n",
    "1. Zainicjalizuj parametry $\\pmb{\\theta}$ ($\\pmb{\\mu}$, $\\pmb{\\Sigma}$, $\\pmb{\\pi}$)\n",
    "1. Oblicz *responsibilities* $\\gamma\\left(z_{nk}\\right)$ dla aktualnych wartości parametrów (*expectation step*)\n",
    "1. Oblicz i zaktualizuj parametry ($\\pmb{\\mu}$, $\\pmb{\\Sigma}$, $\\pmb{\\pi}$) wykorzystując wartości *responsibilities* (*maximization step*).\n",
    "\n",
    "Algorytm jest zatrzymywany po osiągnięciu określonej liczby kroków, gdy zmiana log-likelihood jest poniżej progu lub gdy zmiana parametrów jest poniżej progu.\n",
    "\n",
    "W tym laboratorium zadanie będzie polegało na przygotowaniu i analizie algorytmu uczenia modelu mikstur rozkładów normalnych *Expectation-Maximization*. Wykorzystamy w tym celu dwa dwuwymiarowe zbiory: przykładowy zbiór *blobów* `toy_dataset` oraz rzeczywisty, wielomodalny zbiorze `s_data` (źródło: http://cs.joensuu.fi/sipu/datasets/). W ogólności jednak, GMM-y mogą być stosowane do wielowymiarowych danych.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-sugar",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28059e242a94a05c75f9c1a9ea970ec5",
     "grade": false,
     "grade_id": "toy-dataset",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# zbiór do sanity check\n",
    "blob, _ = make_blobs(\n",
    "    500,\n",
    "    random_state=0, \n",
    "    centers=((-3, -2), (4, 3), (2.5, -1.8)), \n",
    "    cluster_std=(0.7, 1.2, 0.9)\n",
    ")\n",
    "blob = (blob - blob.mean(axis=0)) / blob.std(axis=0)\n",
    "\n",
    "plt.scatter(blob[:, 0], blob[:, 1], s=4)\n",
    "plt.show()\n",
    "\n",
    "toy_data = torch.from_numpy(blob).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-intensity",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9d58871ab635cb724707fe94976c58b",
     "grade": false,
     "grade_id": "s-set",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# zbiór złożony\n",
    "s_set = load_s_set()\n",
    "\n",
    "plt.scatter(s_set[\"x\"], s_set[\"y\"], s=4)\n",
    "plt.show()\n",
    "\n",
    "s_data = torch.from_numpy(s_set.to_numpy()).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-variance",
   "metadata": {},
   "source": [
    "## Zadanie 1. (2,5 p.)\n",
    "\n",
    "Zaimplementuj etapy **Expectation** oraz **Maximization**, a także obliczanie **log-likelihood** dla dwuwymiarowego modelu mikstur rozkładów normalnych, używając metod z biblioteki `torch`. Wykorzystaj podane wzory oraz wiedzę z wykładu. Zwróć uwagę na efektywność przygotowywanego kodu. Sprawdź, czy Twoje funkcje zwracają poprawne wymiary danych używając testów. Następnie wykonaj komórki z uczeniem modelu oraz przygotuj animację uczenia dla zbioru `toy_data` oraz `s_data`; sprawdź, jak liczba komponentów wpływa na działanie modelu.\n",
    "\n",
    "Animacja oraz wyświetlanie historii log-likelihood, jak również metoda uczenia modelu `fit` i jego inicjalizacja zostały już zaimplementowane.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-leone",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95a25f83936743f197f6fe379d54c3e4",
     "grade": false,
     "grade_id": "expectation-maximization",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GaussianMixtureModel2d:\n",
    "    distribution = dist.MultivariateNormal\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_components: int, \n",
    "        n_epochs: int = 50,\n",
    "        termination_threshold: float = 1e-3, \n",
    "        verbose: bool = True\n",
    "    ):\n",
    "        \"\"\"Inicjalizacja modelu dwuwymiarowego.\n",
    "        \n",
    "        :param n_components: liczba komponentów modelu\n",
    "        :param n_epochs: liczba epok trenowania modelu\n",
    "        :param termination_threshold: próg przyrostu log likelihood do zatrzymywania\n",
    "        :param verbose: wyświetl informację o trenowaniu\n",
    "        \"\"\"\n",
    "        self.history = {\n",
    "            \"means\": [],\n",
    "            \"covariances\": [],\n",
    "            \"mixing_coefs\": [],\n",
    "            \"log_likelihood\": []\n",
    "        }\n",
    "        \n",
    "        self.n_components = n_components\n",
    "        self.n_epochs = n_epochs\n",
    "        self.dim = 2\n",
    "        self.termination_threshold = termination_threshold\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.means = torch.rand((self.n_components, self.dim))\n",
    "        self.covariances = torch.eye(self.dim).repeat(\n",
    "            self.n_components, 1, 1\n",
    "        )\n",
    "        self.mixing_coefs = torch.full(\n",
    "            (self.n_components,),\n",
    "            fill_value=1 / self.n_components\n",
    "        )\n",
    "        \n",
    "    def expectation_step(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Krok Expectation (obliczenie responsibilities).\"\"\"\n",
    "        # oblicz responsibilities zgodnie z równaniem\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "        return responsibilities\n",
    "        \n",
    "    \n",
    "    def maximization_step(\n",
    "        self, x: torch.Tensor, responsibilities: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"Krok Maximization (obliczenie parametrów).\"\"\"\n",
    "        # zaktualizuj parametry modelu zgodnie z równaniami\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "        self.means = means\n",
    "        self.covariances = covariances\n",
    "        self.mixing_coefs = mixing_coefs\n",
    "        \n",
    "    def loglikelihood(self, x: torch.Tensor) -> float:\n",
    "        \"\"\"Log-likelihood modelu.\"\"\"\n",
    "        # oblicz średni log likelihood zgodnie z równaniem\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "        return avg_logprob\n",
    "    \n",
    "    def update_history(self, x: torch.Tensor):\n",
    "        \"\"\"Aktualizacja logu.\"\"\"\n",
    "        self.history[\"means\"].append(self.means)\n",
    "        self.history[\"covariances\"].append(self.covariances)\n",
    "        self.history[\"mixing_coefs\"].append(self.mixing_coefs)\n",
    "        self.history[\"log_likelihood\"].append(self.loglikelihood(x))\n",
    "    \n",
    "    def fit(self, x: torch.Tensor) -> float:\n",
    "        \"\"\"Trenowanie modelu.\"\"\"\n",
    "        self.update_history(x)\n",
    "        for epoch in trange(self.n_epochs):\n",
    "            responsibilities = self.expectation_step(x)\n",
    "            self.maximization_step(x, responsibilities)\n",
    "            self.update_history(x)\n",
    "            if (\n",
    "                self.history[\"log_likelihood\"][-1] - self.history[\"log_likelihood\"][-2] \n",
    "                < self.termination_threshold\n",
    "            ):\n",
    "                print(\"Log likelihood not increasing, stopping.\")\n",
    "                self.n_epochs = epoch\n",
    "                break\n",
    "        \n",
    "        if self.verbose:\n",
    "            self._plot_training()\n",
    "        \n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predykcja posterior dla każdego z komponentów.\"\"\"\n",
    "        return self.expectation_step\n",
    "        \n",
    "    def _plot_training(self):\n",
    "        \"\"\"Wizualizacja uczenia.\"\"\"\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(self.history[\"log_likelihood\"])\n",
    "        ax.set_xlabel(\"Epoka\")\n",
    "        ax.set_ylabel(\"Log likelihood\")\n",
    "        plt.show()\n",
    "        \n",
    "    def animation(self, x: torch.Tensor):\n",
    "        \"\"\"Animacja uczenia modelu.\"\"\"\n",
    "        fig, axes = visualize(points=x.numpy())\n",
    "        anim = animation.FuncAnimation(\n",
    "            fig=fig,\n",
    "            func=lambda i: visualize(\n",
    "                points=x.numpy(),\n",
    "                gaussians=[\n",
    "                    (mu, sigma, pi)\n",
    "                    for mu, sigma, pi \n",
    "                    in zip(\n",
    "                        self.history[\"means\"][i].numpy(),\n",
    "                        self.history[\"covariances\"][i].numpy(),\n",
    "                        self.history[\"mixing_coefs\"][i].numpy(),\n",
    "                    )\n",
    "                ],\n",
    "                fig=fig\n",
    "            ), \n",
    "            frames=range(0, self.n_epochs, 5 if self.n_epochs > 10 else 1), \n",
    "            interval=500,\n",
    "            blit=False\n",
    "        )\n",
    "        plt.close()\n",
    "        return anim.to_jshtml()\n",
    "    \n",
    "    def sample(\n",
    "        self, sample_shape: torch.Size = torch.Size([])\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Próbkuj z modelu mikstur.\"\"\"\n",
    "        # wylosuj komponent, a następnie próbkuj z komponentu\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-yeast",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f040084a47e3f19ed5868b7e221d704",
     "grade": true,
     "grade_id": "expectation-maximization-tests",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_almost_equal\n",
    "\n",
    "n_components = 3\n",
    "n_examples = 5\n",
    "test_x = torch.rand(n_examples, 2)\n",
    "\n",
    "# parametry modelu\n",
    "test_model = GaussianMixtureModel2d(n_components, n_epochs=1, verbose=False)\n",
    "means_shape = test_model.means.shape\n",
    "covariances_shape = test_model.covariances.shape\n",
    "mixing_coefs_shape = test_model.mixing_coefs.shape\n",
    "assert test_model.n_components == n_components\n",
    "assert means_shape == (n_components, 2)\n",
    "assert covariances_shape == (n_components, 2, 2)\n",
    "assert mixing_coefs_shape == (n_components,)\n",
    "\n",
    "log_likelihood_1 = test_model.loglikelihood(test_x)\n",
    "\n",
    "# responsibilities\n",
    "responsibilities = test_model.expectation_step(test_x)\n",
    "assert responsibilities.numel() == n_components * n_examples\n",
    "assert type(responsibilities) == torch.Tensor\n",
    "\n",
    "# parametry po aktualizacji\n",
    "test_model.maximization_step(test_x, responsibilities)\n",
    "assert test_model.means.shape == means_shape\n",
    "assert test_model.covariances.shape == covariances_shape\n",
    "assert test_model.mixing_coefs.shape == mixing_coefs_shape\n",
    "\n",
    "log_likelihood_2 = test_model.loglikelihood(test_x)\n",
    "\n",
    "assert log_likelihood_1 < log_likelihood_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-booth",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d94ca80da9b131125703457e06040989",
     "grade": false,
     "grade_id": "train-gmm2d-toy",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = GaussianMixtureModel2d(3, n_epochs=50)\n",
    "model.fit(toy_data)\n",
    "if not grading:\n",
    "    display(HTML(model.animation(toy_data)))\n",
    "else:\n",
    "    visualize(\n",
    "        points=toy_data.numpy(),\n",
    "        gaussians=[\n",
    "            (mu, sigma, pi)\n",
    "            for mu, sigma, pi \n",
    "            in zip(\n",
    "                model.means.numpy(),\n",
    "                model.covariances.numpy(),\n",
    "                model.mixing_coefs.numpy(),\n",
    "            )\n",
    "        ],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-alert",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "480de86d24594ebcc9c20ce479e7c854",
     "grade": false,
     "grade_id": "train-gmm2d-sset",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = GaussianMixtureModel2d(5, n_epochs=50)\n",
    "model.fit(s_data)\n",
    "if not grading:\n",
    "    display(HTML(model.animation(s_data)))\n",
    "else:\n",
    "    visualize(\n",
    "        points=s_data.numpy(),\n",
    "        gaussians=[\n",
    "            (mu, sigma, pi)\n",
    "            for mu, sigma, pi \n",
    "            in zip(\n",
    "                model.means.numpy(),\n",
    "                model.covariances.numpy(),\n",
    "                model.mixing_coefs.numpy(),\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-lotus",
   "metadata": {},
   "source": [
    "## Zadanie 2. (0,5 p.)\n",
    "\n",
    "Losowy dobór początkowych średnich dla wszystkich komponentów może nie być najlepszym rozwiazaniem - modele mikstur są podatne na lokalne ekstrema. Jedną z interesujących metod inicjalizacji średnich modelu jest wykorzystanie klasteryzacji K-Means.\n",
    "\n",
    "Użyj implementacji [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) z biblioteki scikit-learn do zainicjalizowania średnich każdego z komponentów modelu. Zadanie wykonaj tworząc klasę dziedziczącą po oryginalnej klasie GMM. Sprawdź działanie modelu dla obu zbiorów danych i porównaj z modelem inicjalizowanym losowo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-cattle",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "421c14d551d81feb9015b0f27908ebd8",
     "grade": true,
     "grade_id": "k-means-init-solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GaussianMixtureModel2dKMeans(GaussianMixtureModel2d):\n",
    "    \"\"\"GMM z inicjalizacją średnich z użyciem KMeans.\"\"\"\n",
    "    # Zainicjalizuj średnie używając KMeans i wywołaj uczenie\n",
    "    # TU WPISZ KOD\n",
    "    raise NotImplementedError()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-citation",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b51d236895c3eccad6aa01e4b2f64a8",
     "grade": false,
     "grade_id": "train-gmm2d-kmeans-toy",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = GaussianMixtureModel2dKMeans(3, n_epochs=20)\n",
    "model.fit(toy_data)\n",
    "if not grading:\n",
    "    display(HTML(model.animation(toy_data)))\n",
    "else:\n",
    "    visualize(\n",
    "        points=toy_data.numpy(),\n",
    "        gaussians=[\n",
    "            (mu, sigma, pi)\n",
    "            for mu, sigma, pi \n",
    "            in zip(\n",
    "                model.means.numpy(),\n",
    "                model.covariances.numpy(),\n",
    "                model.mixing_coefs.numpy(),\n",
    "            )\n",
    "        ],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-economy",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "982fa57a5334429fd382893c9359f049",
     "grade": false,
     "grade_id": "train-gmm2d-kmeans-sset",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = GaussianMixtureModel2dKMeans(5, n_epochs=50)\n",
    "model.fit(s_data)\n",
    "if not grading:\n",
    "    display(HTML(model.animation(s_data)))\n",
    "else:\n",
    "    visualize(\n",
    "        points=s_data.numpy(),\n",
    "        gaussians=[\n",
    "            (mu, sigma, pi)\n",
    "            for mu, sigma, pi \n",
    "            in zip(\n",
    "                model.means.numpy(),\n",
    "                model.covariances.numpy(),\n",
    "                model.mixing_coefs.numpy(),\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-emergency",
   "metadata": {},
   "source": [
    "## Zadanie 3. (1 p.)\n",
    "\n",
    "Model mikstur mogą być wykorzystywane jako modele generujące. Zaimplementuj w klasie nadrzędnej modelu metodę `sample`, która wykona operację próbkowania: najpierw wylosuj komponent, wykorzystując *mixing coefficients*, a następnie dokonaj próbkowania z wybranego rozkładu i zwróć punkt (lub punkty). Porównaj PDF dla mikstury rozkładów oraz po próbkowaniu dużej liczby punktów z modelu (wskazówka: zainspiruj się kodem z funkcji `visualize` by przygotować wizualizację 2D i 3D oraz wykorzystaj metodę `np.histogram2d` dla wypróbkowanych punktów - zwróć uwagę na osie x i y, które są w niej zamienione).\n",
    "\n",
    "UWAGA: zalecamy, by liczba próbkowań nie była większa niż `100_000` (przy założeniu użycia wektoryzowanego próbkowania z `torch`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-session",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c769162bb9ebe571909d3e37cde1ddb8",
     "grade": true,
     "grade_id": "sample-solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax1 = fig.add_subplot(221)                   # wykres 2D dla mikstury rozkładów\n",
    "ax2 = fig.add_subplot(222, projection=\"3d\")  # wykres 3D dla mikstury rozkładów\n",
    "ax3 = fig.add_subplot(223)                   # wykres 2D dla wypróbkowanych punktów\n",
    "ax4 = fig.add_subplot(224, projection=\"3d\")  # wykres 3D dla wypróbkowanych punktów\n",
    "\n",
    "min_val, max_val = -2, 2                     # zakres dla osi X i Y\n",
    "n_bins = 21                                  # liczba przedziałów dla histogramu oraz wizualizacji mikstury rozkładów\n",
    "x = np.linspace(min_val, max_val, n_bins)\n",
    "y = np.linspace(min_val, max_val, n_bins)\n",
    "xs, ys = np.meshgrid(x, y)\n",
    "\n",
    "# TU WPISZ KOD\n",
    "raise NotImplementedError()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-commons",
   "metadata": {},
   "source": [
    "## Dobór liczby komponentów\n",
    "\n",
    "Dotychczas wyboru liczby komponentów w miksturze dokonywaliśmy empirycznie. Podejściem bardziej metodycznym byłoby przygotowanie modeli o różnej liczbie komponentów i sprawdzenie, który z nich osiąga najwyższą wartość likelihood dla danych. Alternatywnie, dobór liczby komponentów można by oprzeć na kryterium analitycznym. W literaturze najczęściej wykorzystuje się **Akaike Information Criterion (AIC)** (równanie $(AIC)$) oraz **Bayesian Information Criterion (BIC)** (równanie $(BIC)$). Również i tutaj wybór modelu oparty jest na maksymalnej wartości kryterium.\n",
    "\n",
    "$$\\mathit{AIC}=\\ln\\left(\\hat{L}\\right) - M\\tag{AIC}$$\n",
    "\n",
    "$$\\mathit{BIC}=\\ln\\left(\\hat{L}\\right) - \\frac{1}{2} M \\ln N\\tag{BIC}$$\n",
    "\n",
    "gdzie $M$ to liczba estymowanych parametrów w modelu, $N$ to liczba punktów w danych, a $\\ln\\left(\\hat{L}\\right)$ to najwyższa wartość log-likelihood. Porównując AIC i BIC widać, że BIC bardziej penalizuje złożoność modelu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-bailey",
   "metadata": {},
   "source": [
    "## Zadanie 4. (1 p.)\n",
    "\n",
    "Zaimplementuj metryki AIC i BIC według wzorów. Następnie zbadaj, dla jakiej liczby komponentów z przedziału $\\{1, ..., 15\\}$ GMM osiąga najlepsze wartości obu kryteriów oraz log-likelihood dla zbioru `s_set`. Zapisz swoje obserwacje.\n",
    "\n",
    "UWAGA: tutaj za likelihood bierzemy sumę dla wszystkich obserwacji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-ukraine",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e9096cfbe89c640545866c5f579a1ca",
     "grade": false,
     "grade_id": "components-solution",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def aic(x: torch.Tensor, model: GaussianMixtureModel2d) -> float:\n",
    "    \"\"\"Oblicz wartość AIC dla danych i parametrów modelu.\"\"\"\n",
    "    # TU WPISZ KOD\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def bic(x: torch.Tensor, model: GaussianMixtureModel2d) -> float:\n",
    "    \"\"\"Oblicz wartość BIC dla danych i parametru modelu.\"\"\"\n",
    "    # TU WPISZ KOD\n",
    "    raise NotImplementedError()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-albany",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7b358995f8fc0d384a921f5c7d5d6fc",
     "grade": true,
     "grade_id": "components-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_almost_equal\n",
    "\n",
    "test_model.fit(test_x)\n",
    "assert type(aic(test_x, test_model)) == float\n",
    "assert type(bic(test_x, test_model)) == float\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-marketing",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4527c4f5d77e370b5784dd0119aeed6a",
     "grade": false,
     "grade_id": "components-visualization",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not grading:\n",
    "    log_dict = {\"components\": [], \"aic\": [], \"bic\": [], \"log_likelihood\": []}\n",
    "    for n_components in range(1, 16):\n",
    "        print(f\"Training for {n_components} components\")\n",
    "        model = GaussianMixtureModel2dKMeans(n_components, n_epochs=100, verbose=False)\n",
    "        model.fit(s_data)\n",
    "        log_dict[\"components\"].append(n_components)\n",
    "        log_dict[\"log_likelihood\"].append(s_data.shape[0] * max(model.history[\"log_likelihood\"]))\n",
    "        log_dict[\"aic\"].append(aic(s_data, model))\n",
    "        log_dict[\"bic\"].append(bic(s_data, model))\n",
    "\n",
    "    df = pd.DataFrame(log_dict)\n",
    "    df.plot(\"components\", [\"aic\", \"bic\", \"log_likelihood\"])\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
